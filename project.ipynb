{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f783b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Section 1 - Initialization: Imports\n",
    "#\n",
    "# Import all required libraries for the project.\n",
    "#\n",
    "\n",
    "# --- Core Libraries ---\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# --- Data Handling ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "# --- PySpark Core ---\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when, isnan, avg, stddev, min, max, sum, lit, broadcast\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# --- PySpark ML (Preprocessing) ---\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "# --- PySpark ML (Models) ---\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
    "# from pyspark.ml.clustering import IsolationForest # Note: Requires Spark 3.0+ \n",
    "from sklearn.ensemble import IsolationForest\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# --- Visualization ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Model Evaluation (Local) ---\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Cell 1: All libraries imported successfully.\")\n",
    "print(f\"PySpark Version: {pyspark.__version__}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaf65a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Section 1 - Logging and Path Configuration\n",
    "#\n",
    "# Configure logging, define all project paths, and create output directories.\n",
    "#\n",
    "\n",
    "# --- 1. Configure Logging ---\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "log = logging.getLogger(\"FraudDetectionLogger\")\n",
    "log.info(\"Logging initialized.\")\n",
    "\n",
    "# --- 2. Define Project Paths ---\n",
    "try:\n",
    "    # Get the user's home directory \n",
    "    HOME_DIR = '/home/smitvd22' \n",
    "    \n",
    "    # Base project directory\n",
    "    PROJECT_DIR = os.path.join(HOME_DIR, 'SparkProject')\n",
    "    \n",
    "    # --- Local Paths (for initial ingest) ---\n",
    "    LOCAL_DATASET_PATH = os.path.join(PROJECT_DIR, 'dataset', 'PS_20174392719_1491204439457_log.csv')\n",
    "    \n",
    "    # --- HDFS Paths (for all processing) ---\n",
    "    HDFS_BASE_PATH = f\"/user/smitvd22/fraud_detection\"\n",
    "    HDFS_DATA_PATH = f\"{HDFS_BASE_PATH}/data/transactions.parquet\"\n",
    "    \n",
    "    # --- *** NEW: HDFS Paths for intermediate DataFrames *** ---\n",
    "    HDFS_INTERMEDIATE_PATH = f\"{HDFS_BASE_PATH}/intermediate\"\n",
    "    HDFS_PROCESSED_PATH = f\"{HDFS_INTERMEDIATE_PATH}/processed.parquet\"\n",
    "    HDFS_UNBALANCED_TRAIN_PATH = f\"{HDFS_INTERMEDIATE_PATH}/unbalanced_train.parquet\"\n",
    "    HDFS_TEST_PATH = f\"{HDFS_INTERMEDIATE_PATH}/test.parquet\"\n",
    "    HDFS_TRAIN_PATH = f\"{HDFS_INTERMEDIATE_PATH}/train_balanced.parquet\"\n",
    "    # --- *** ---\n",
    "    \n",
    "    # --- Hive Configuration ---\n",
    "    HIVE_DB_NAME = \"fraud_detection_db\"\n",
    "    HIVE_TABLE_NAME = \"transactions\"\n",
    "    \n",
    "    # --- Output & Model Paths ---\n",
    "    OUTPUT_DIR = os.path.join(PROJECT_DIR, 'output')\n",
    "    PLOT_OUTPUT_DIR = os.path.join(OUTPUT_DIR, 'plots')\n",
    "    METRICS_OUTPUT_DIR = os.path.join(OUTPUT_DIR, 'metrics')\n",
    "    MODEL_OUTPUT_DIR = os.path.join(PROJECT_DIR, 'models') # Save models to local FS\n",
    "    \n",
    "    log.info(f\"Project Directory: {PROJECT_DIR}\")\n",
    "    log.info(f\"Local Dataset: {LOCAL_DATASET_PATH}\")\n",
    "    log.info(f\"HDFS Parquet Path: {HDFS_DATA_PATH}\")\n",
    "    log.info(f\"HDFS Intermediate Path: {HDFS_INTERMEDIATE_PATH}\")\n",
    "\n",
    "    # --- 3. Create Local Output Directories ---\n",
    "    os.makedirs(PLOT_OUTPUT_DIR, exist_ok=True)\n",
    "    os.makedirs(METRICS_OUTPUT_DIR, exist_ok=True)\n",
    "    os.makedirs(MODEL_OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    log.info(f\"Created output plot directory: {PLOT_OUTPUT_DIR}\")\n",
    "\n",
    "    # --- 4. Visualization Settings ---\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.rcParams['figure.dpi'] = 300\n",
    "    log.info(\"Seaborn and Matplotlib configured.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"Error setting up paths: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Cell 2: Logging and paths configured.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da41d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Section 1 - Initialize SparkSession\n",
    "#\n",
    "# Start the SparkSession with Hive support and configurations \n",
    "# for processing a multi-GB dataset.\n",
    "#\n",
    "\n",
    "try:\n",
    "    log.info(\"Building SparkSession...\")\n",
    "    \n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"PySpark_Fraud_Detection_Project\")\n",
    "        .enableHiveSupport()  # Enable integration with Hive Metastore\n",
    "        \n",
    "        # --- *** MODIFICATION: Reduced memory for VM *** ---\n",
    "        # Allocate 2GB to the driver (Jupyter)\n",
    "        .config(\"spark.driver.memory\", \"2g\")\n",
    "        # Allocate 2GB to each executor\n",
    "        .config(\"spark.executor.memory\", \"2g\")\n",
    "        # ---\n",
    "        \n",
    "        # Increase shuffle partitions for better parallelism\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "        # Use a more efficient serializer\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "        \n",
    "        # --- *** MODIFICATION: Reduced max result size *** ---\n",
    "        .config(\"spark.driver.maxResultSize\", \"1g\")\n",
    "        \n",
    "        .getOrCreate()\n",
    "    )\n",
    "    \n",
    "    sc = spark.sparkContext\n",
    "    \n",
    "    log.info(\"SparkSession created successfully.\")\n",
    "    log.info(f\"Spark UI available at: {sc.uiWebUrl}\")\n",
    "\n",
    "    # --- Create Hive Database ---\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {HIVE_DB_NAME}\")\n",
    "    spark.sql(f\"USE {HIVE_DB_NAME}\")\n",
    "    log.info(f\"Hive database '{HIVE_DB_NAME}' created and in use.\")\n",
    "    \n",
    "    # --- *** NEW: Create HDFS directory for intermediate files *** ---\n",
    "    # Get the Hadoop FileSystem object\n",
    "    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "    # Create the path object\n",
    "    hdfs_path = spark._jvm.org.apache.hadoop.fs.Path(HDFS_INTERMEDIATE_PATH)\n",
    "    # Create the directory\n",
    "    if not fs.exists(hdfs_path):\n",
    "        fs.mkdirs(hdfs_path)\n",
    "        log.info(f\"Created HDFS directory: {HDFS_INTERMEDIATE_PATH}\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"Error initializing SparkSession: {e}\")\n",
    "    # Stop the notebook if Spark can't start\n",
    "    raise e\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Cell 3: SparkSession initialized with Hive support.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55fdd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Section 2 - Data Ingest (Local CSV -> HDFS Parquet)\n",
    "#\n",
    "# Phase 1: Load the large local CSV, define the schema,\n",
    "# and write it to HDFS in the optimized Parquet format.\n",
    "#\n",
    "\n",
    "try:\n",
    "    log.info(f\"Starting Phase 1: Ingesting local CSV from {LOCAL_DATASET_PATH}\")\n",
    "    \n",
    "    # Define the schema to avoid slow 'inferSchema' on 6M rows\n",
    "    schema = StructType([\n",
    "        StructField(\"step\", IntegerType(), True),\n",
    "        StructField(\"type\", StringType(), True),\n",
    "        StructField(\"amount\", DoubleType(), True),\n",
    "        StructField(\"nameOrig\", StringType(), True),\n",
    "        StructField(\"oldbalanceOrg\", DoubleType(), True),\n",
    "        StructField(\"newbalanceOrig\", DoubleType(), True),\n",
    "        StructField(\"nameDest\", StringType(), True),\n",
    "        StructField(\"oldbalanceDest\", DoubleType(), True),\n",
    "        StructField(\"newbalanceDest\", DoubleType(), True),\n",
    "        StructField(\"isFraud\", IntegerType(), True),\n",
    "        StructField(\"isFlaggedFraud\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "    # Read from LOCAL filesystem (note the 'file://' prefix)\n",
    "    df_raw = spark.read.csv(\n",
    "        f\"file://{LOCAL_DATASET_PATH}\",\n",
    "        header=True,\n",
    "        schema=schema\n",
    "    )\n",
    "    \n",
    "    log.info(\"Local CSV read successfully. Writing to HDFS Parquet...\")\n",
    "\n",
    "    # Write to HDFS in Parquet format. This is much faster for analysis.\n",
    "    # 'overwrite' mode allows us to re-run this notebook\n",
    "    df_raw.write.mode(\"overwrite\").parquet(HDFS_DATA_PATH)\n",
    "    \n",
    "    log.info(f\"Phase 1 Complete: Data written to HDFS at {HDFS_DATA_PATH}\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"Error during data ingest: {e}\")\n",
    "    log.error(\"Did you start HDFS? (e.g., /hadoop-2.6.5/sbin/start-dfs.sh)\")\n",
    "    raise e\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Cell 4: Data ingested from local CSV and saved to HDFS as Parquet.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b0496c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Section 2 - Load Data from HDFS\n",
    "#\n",
    "# Phase 2: All subsequent work will use the fast, distributed HDFS Parquet file.\n",
    "#\n",
    "\n",
    "try:\n",
    "    log.info(f\"Starting Phase 2: Loading Parquet data from HDFS: {HDFS_DATA_PATH}\")\n",
    "    \n",
    "    df = spark.read.parquet(HDFS_DATA_PATH)\n",
    "    \n",
    "    # --- *** MODIFICATION: Removed persist() *** ---\n",
    "    # We will re-read this file from HDFS as needed.\n",
    "    \n",
    "    # Trigger an action to load the data and log the count\n",
    "    total_records = df.count()\n",
    "    log.info(f\"Successfully loaded {total_records} records from HDFS Parquet.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"Error loading HDFS Parquet file: {e}\")\n",
    "    raise e\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"Cell 5: Successfully loaded {total_records} records from HDFS.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfde79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Section 2 - Schema and First Look\n",
    "try:\n",
    "    log.info(\"Displaying DataFrame schema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    log.info(\"Displaying first 20 records:\")\n",
    "    df.show(20, truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    log.error(f\"Error displaying schema or data: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Cell 6: Schema and sample data displayed.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0202174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Section 2 - Null Value Check\n",
    "#\n",
    "# Check for null or NaN values in all columns.\n",
    "#\n",
    "\n",
    "try:\n",
    "    log.info(\"Checking for null values...\")\n",
    "    \n",
    "    # Create an expression for each column to count nulls\n",
    "    null_counts = df.select([\n",
    "        count(when(isnan(c) | col(c).isNull(), c)).alias(c) \n",
    "        for c in df.columns\n",
    "    ])\n",
    "    \n",
    "    log.info(\"Null value counts per column:\")\n",
    "    null_counts.show()\n",
    "    \n",
    "    # This will be a 1-row DataFrame. Collect it to print a summary.\n",
    "    null_counts_pd = null_counts.collect()[0].asDict()\n",
    "    total_nulls = sum(null_counts_pd.values())\n",
    "    \n",
    "    if total_nulls == 0:\n",
    "        log.info(\"PASSED: No null or NaN values found in the dataset.\")\n",
    "    else:\n",
    "        log.warning(f\"WARNING: Found {total_nulls} total null/NaN values.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Note: The 'sum' error you saw before was from Python's sum on a Spark object\n",
    "    # This cell uses 'sum' on a Python dict, which is correct.\n",
    "    log.error(f\"Error checking for null values: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Cell 7: Null value check complete.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f91067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Section 2 - Basic Statistics\n",
    "#\n",
    "# Calculate descriptive statistics for key numerical columns.\n",
    "#\n",
    "\n",
    "try:\n",
    "    log.info(\"Calculating descriptive statistics...\")\n",
    "    \n",
    "    numeric_cols = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "    \n",
    "    stats = df.describe(numeric_cols)\n",
    "    stats.show()\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"Error calculating statistics: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Cell 8: Descriptive statistics calculated.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400dcdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Section 3 (EDA) - Create Hive Table\n",
    "#\n",
    "# Create a managed Hive table for SQL-based analysis.\n",
    "#\n",
    "\n",
    "try:\n",
    "    log.info(f\"Creating Hive table '{HIVE_TABLE_NAME}' in database '{HIVE_DB_NAME}'...\")\n",
    "    \n",
    "    # Save the DataFrame as a managed table in Hive\n",
    "    df.write.mode(\"overwrite\").saveAsTable(f\"{HIVE_DB_NAME}.{HIVE_TABLE_NAME}\")\n",
    "    \n",
    "    log.info(\"Hive table created successfully.\")\n",
    "    \n",
    "    # Verify by showing tables\n",
    "    spark.sql(f\"USE {HIVE_DB_NAME}\")\n",
    "    spark.sql(\"SHOW TABLES\").show()\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"Error creating Hive table: {e}\")\n",
    "    raise e\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"Cell 9: Hive table '{HIVE_TABLE_NAME}' created.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3ac4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Section 3 (EDA) - Class Imbalance (SQL)\n",
    "#\n",
    "# Analyze the distribution of fraud vs. normal transactions.\n",
    "#\n",
    "\n",
    "try:\n",
    "    log.info(\"Analyzing class imbalance using Hive SQL...\")\n",
    "    \n",
    "    sql_query = f\"\"\"\n",
    "    SELECT \n",
    "        isFraud, \n",
    "        COUNT(*) as count,\n",
    "        (COUNT(*) / (SELECT COUNT(*) FROM {HIVE_TABLE_NAME})) * 100 as percentage\n",
    "    FROM {HIVE_TABLE_NAME}\n",
    "    GROUP BY isFraud\n",
    "    \"\"\"\n",
    "    \n",
    "    imbalance_df = spark.sql(sql_query)\n",
    "    imbalance_df.show()\n",
    "    \n",
    "    # Collect the small result to calculate imbalance ratio\n",
    "    imbalance_data = imbalance_df.collect()\n",
    "    \n",
    "    count_normal = 0\n",
    "    count_fraud = 0\n",
    "    \n",
    "    for row in imbalance_data:\n",
    "        if row['isFraud'] == 0:\n",
    "            count_normal = row['count']\n",
    "        else:\n",
    "            count_fraud = row['count']\n",
    "            \n",
    "    fraud_rate = (count_fraud / total_records) * 100\n",
    "    imbalance_ratio = count_normal / count_fraud\n",
    "    \n",
    "    log.info(f\"Total Records: {total_records}\")\n",
    "    log.info(f\"Normal (0): {count_normal} | Fraud (1): {count_fraud}\")\n",
    "    log.info(f\"Fraud Rate: {fraud_rate:.4f}%\")\n",
    "    log.info(f\"Imbalance Ratio (Normal:Fraud): {imbalance_ratio:.2f} : 1\")\n",
    "    \n",
    "    # Save for plotting\n",
    "    plot_data_imbalance = imbalance_df.toPandas()\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"Error analyzing class imbalance: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"Cell 10: Class imbalance analyzed. Fraud Rate: {fraud_rate:.4f}%.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef51002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Section 3 (EDA) - Class Imbalance (Visualization)\n",
    "\n",
    "try:\n",
    "    log.info(\"Plotting class distribution...\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    ax = sns.barplot(\n",
    "        x=plot_data_imbalance['isFraud'].map({0: 'Normal', 1: 'Fraud'}), \n",
    "        y=plot_data_imbalance['percentage']\n",
    "    )\n",
    "    \n",
    "    ax.set_title('Class Distribution (Fraud vs. Normal)', fontsize=16)\n",
    "    ax.set_xlabel('Transaction Class', fontsize=12)\n",
    "    ax.set_ylabel('Percentage of Total Transactions (%)', fontsize=12)\n",
    "    \n",
    "    # Add text labels\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(\n",
    "            f\"{p.get_height():.4f}%\", \n",
    "            (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "            ha='center', va='center', \n",
    "            xytext=(0, 9), \n",
    "            textcoords='offset points'\n",
    "        )\n",
    "    \n",
    "    plot_path = os.path.join(PLOT_OUTPUT_DIR, '01_class_distribution.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    log.info(f\"Saved class distribution plot to {plot_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"Error plotting class distribution: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Cell 11: Class distribution plot generated.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64765974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Section 3 (EDA) - Fraud by Transaction Type (SQL)\n",
    "\n",
    "try:\n",
    "    log.info(\"Analyzing fraud statistics by transaction type using Hive SQL...\")\n",
    "    \n",
    "    sql_query = f\"\"\"\n",
    "    SELECT \n",
    "        type, \n",
    "        COUNT(*) as total_transactions,\n",
    "        SUM(isFraud) as total_fraud,\n",
    "        (SUM(isFraud) / COUNT(*)) * 100 as fraud_percentage_of_type,\n",
    "        (SUM(isFraud) / (SELECT SUM(isFraud) FROM {HIVE_TABLE_NAME})) * 100 as percentage_of_all_fraud\n",
    "    FROM {HIVE_TABLE_NAME}\n",
    "    GROUP BY type\n",
    "    ORDER BY total_fraud DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    type_fraud_df = spark.sql(sql_query)\n",
    "    \n",
    "    log.info(\"Fraud Statistics by Transaction Type:\")\n",
    "    type_fraud_df.show()\n",
    "    \n",
    "    # Key Insight\n",
    "    log.warning(\"KEY INSIGHT: Note which transaction types have 0 fraud.\")\n",
    "    \n",
    "    # Save for plotting\n",
    "    plot_data_type = type_fraud_df.toPandas()\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"Error analyzing fraud by type: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Cell 12: Fraud statistics by transaction type calculated.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046ec0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Section 3 (EDA) - Fraud by Type (Visualization)\n",
    "\n",
    "try:\n",
    "    log.info(\"Plotting fraud by transaction type...\")\n",
    "    \n",
    "    # Filter for plotting, as non-fraud types dominate\n",
    "    plot_data_type_fraud = plot_data_type[plot_data_type['total_fraud'] > 0]\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "    # Bar plot for total fraud count\n",
    "    sns.barplot(\n",
    "        x='type', \n",
    "        y='total_fraud', \n",
    "        data=plot_data_type_fraud, \n",
    "        ax=ax1, \n",
    "        color='orange'\n",
    "    )\n",
    "    ax1.set_xlabel('Transaction Type', fontsize=12)\n",
    "    ax1.set_ylabel('Total Fraudulent Transactions (Count)', fontsize=12, color='orange')\n",
    "    ax1.tick_params(axis='y', labelcolor='orange')\n",
    "\n",
    "    # Line plot for fraud percentage\n",
    "    ax2 = ax1.twinx()\n",
    "    sns.lineplot(\n",
    "        x='type', \n",
    "        y='fraud_percentage_of_type', \n",
    "        data=plot_data_type_fraud, \n",
    "        ax=ax2, \n",
    "        color='blue', \n",
    "        marker='o'\n",
    "    )\n",
    "    ax2.set_ylabel('Fraud as % of Type', fontsize=12, color='blue')\n",
    "    ax2.tick_params(axis='y', labelcolor='blue')\n",
    "    ax2.set_ylim(0, 100) # Percentage\n",
    "    \n",
    "    plt.title('Fraud Analysis by Transaction Type', fontsize=16)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    plot_path = os.path.join(PLOT_OUTPUT_DIR, '02_fraud_by_type.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    log.info(f\"Saved fraud by type plot to {plot_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"Error plotting fraud by type: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Cell 13: Fraud by transaction type plot generated.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319963f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Section 3 (EDA) - Amount Distribution (Pandas Sample)\n",
    "#\n",
    "# Sample data to plot distributions, as plotting 6.3M points is too slow.\n",
    "#\n",
    "\n",
    "try:\n",
    "    log.info(\"Analyzing transaction amount distributions...\")\n",
    "    \n",
    "    # Get all fraud transactions (small enough to collect)\n",
    "    df_fraud = df.filter(col(\"isFraud\") == 1).toPandas()\n",
    "    log.info(f\"Fraud Amount Stats:\\n{df_fraud['amount'].describe()}\")\n",
    "    \n",
    "    # Get a sample of normal transactions\n",
    "    df_normal_sample = df.filter(col(\"isFraud\") == 0).sample(False, 0.01, seed=42).toPandas()\n",
    "    log.info(f\"Normal Amount Stats (from 1% sample):\\n{df_normal_sample['amount'].describe()}\")\n",
    "    \n",
    "    # Plotting (with log scale for better visibility)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(\n",
    "        np.log1p(df_normal_sample['amount']), \n",
    "        bins=50, \n",
    "        kde=True, \n",
    "        color='blue', \n",
    "        label='Normal (1% Sample)',\n",
    "        stat='density'\n",
    "    )\n",
    "    sns.histplot(\n",
    "        np.log1p(df_fraud['amount']), \n",
    "        bins=50, \n",
    "        kde=True, \n",
    "        color='red', \n",
    "        label='Fraud',\n",
    "        stat='density'\n",
    "    )\n",
    "    \n",
    "    plt.title('Log(1 + Amount) Distribution by Class', fontsize=16)\n",
    "    plt.xlabel('Log(1 + Amount)', fontsize=12)\n",
    "    plt.ylabel('Density', fontsize=12)\n",
    "    plt.legend()\n",
    "    \n",
    "    plot_path = os.path.join(PLOT_OUTPUT_DIR, '03_amount_distribution_log.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    log.info(f\"Saved amount distribution plot to {plot_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"Error plotting amount distributions: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Cell 14: Amount distribution analysis complete.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97060398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Section 3 (EDA) - Correlation Heatmap (Pandas Sample)\n",
    "#\n",
    "# Calculate correlation matrix on a sample for performance.\n",
    "#\n",
    "\n",
    "try:\n",
    "    log.info(\"Calculating correlation matrix on a 1% data sample...\")\n",
    "    \n",
    "    # Use the Pandas sample we already have\n",
    "    sample_df_pd = df_normal_sample.sample(frac=0.1, random_state=42) # Sample the sample\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    corr = sample_df_pd.corr()\n",
    "    \n",
    "    sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    \n",
    "    plt.title('Correlation Heatmap (1% Sample of Normal Transactions)', fontsize=16)\n",
    "    \n",
    "    plot_path = os.path.join(PLOT_OUTPUT_DIR, '04_correlation_heatmap.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    log.info(f\"Saved correlation heatmap to {plot_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"Error plotting correlation heatmap: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Cell 15: Correlation analysis complete.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5381a7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Section 4 (Feature Engineering) - Drop Unused Columns\n",
    "#\n",
    "# Drop columns that are irrelevant (like 'isFlaggedFraud') or\n",
    "# are complex and not used (like 'nameOrig', 'nameDest').\n",
    "#\n",
    "\n",
    "try:\n",
    "    log.info(\"Starting Feature Engineering: Dropping unused columns...\")\n",
    "    \n",
    "    # We will keep 'step' for time-series analysis\n",
    "    # We keep 'type', 'amount', 'oldbalanceOrg', 'newbalanceOrig', \n",
    "    # 'oldbalanceDest', 'newbalanceDest', and 'isFraud'.\n",
    "    \n",
    "    columns_to_drop = ['nameOrig', 'nameDest', 'isFlaggedFraud']\n",
    "    \n",
    "    df_step1 = df.drop(*columns_to_drop)\n",
    "    \n",
    "    log.info(f\"Dropped columns: {columns_to_drop}\")\n",
    "    df_step1.printSchema()\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"Error dropping columns: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Cell 16: Unused columns dropped.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f674db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Section 4 (Feature Engineering) - Time-Series Features\n",
    "#\n",
    "# Create velocity features (e.g., transactions per hour).\n",
    "#\n",
    "\n",
    "try:\n",
    "    log.info(\"Engineering time-series features (hour_of_day)...\")\n",
    "    \n",
    "    # 'step' represents 1 hour. We can get hour of day.\n",
    "    # Assuming 'step' 1 is the 1st hour of the 1st day.\n",
    "    \n",
    "    df_with_time = df_step1.withColumn(\n",
    "        \"hour_of_day\", \n",
    "        (col(\"step\") - 1) % 24 # -1 to make it 0-indexed (0-23)\n",
    "    )\n",
    "    \n",
    "    log.info(\"Engineered 'hour_of_day'.\")\n",
    "    df_with_time.select(\"step\", \"hour_of_day\").show(5)\n",
    "    \n",
    "    # --- Create Velocity Features (e.g., hourly txn count) ---\n",
    "    # This is a complex window function and a key place for OOM.\n",
    "    # We will skip it for this version as it's not used\n",
    "    # in the final model, but the 'hour_of_day' plot uses it.\n",
    "    #\n",
    "    # We'll rename the df for consistency with the next step.\n",
    "    \n",
    "    df_with_velocity = df_with_time\n",
    "    log.info(\"Skipping complex velocity features for memory.\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"Error engineering time-series features: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Cell 17: Time-series features engineered.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617ddc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Section 4 (Feature Engineering) - Fraud by Hour (SQL)\n",
    "#\n",
    "# This cell was moved from EDA to use the new 'hour_of_day' feature.\n",
    "#\n",
    "\n",
    "try:\n",
    "    log.info(\"Analyzing fraud by 'hour_of_day'...\")\n",
    "    \n",
    "    # We must create a temp view to use SQL on the new DataFrame\n",
    "    df_with_velocity.createOrReplaceTempView(\"transactions_with_time\")\n",
    "    \n",
    "    sql_query = f\"\"\"\n",
    "    SELECT \n",
    "        hour_of_day,\n",
    "        SUM(CASE WHEN isFraud = 1 THEN 1 ELSE 0 END) as total_fraud,\n",
    "        COUNT(*) as total_transactions\n",
    "    FROM transactions_with_time\n",
    "    GROUP BY hour_of_day\n",
    "    ORDER BY hour_of_day\n",
    "    \"\"\"\n",
    "    \n",
    "    hourly_df = spark.sql(sql_query)\n",
    "    \n",
    "    log.info(\"Fraud Statistics by Hour of Day:\")\n",
    "    hourly_df_pd = hourly_df.toPandas()\n",
    "    print(hourly_df_pd)\n",
    "    \n",
    "    # --- Plotting ---\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    ax = sns.barplot(\n",
    "        x='hour_of_day', \n",
    "        y='total_fraud', \n",
    "        data=hourly_df_pd, \n",
    "        color='red'\n",
    "    )\n",
    "    \n",
    "    ax.set_title('Total Fraudulent Transactions by Hour of Day (UTC)', fontsize=16)\n",
    "    ax.set_xlabel('Hour of Day', fontsize=12)\n",
    "    ax.set_ylabel('Total Fraud Count', fontsize=12)\n",
    "    \n",
    "    plot_path = os.path.join(PLOT_OUTPUT_DIR, '05_fraud_by_hour_of_day.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    log.info(f\"Saved peak fraud hours plot to {plot_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"Error analyzing fraud by hour: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Cell 18: Fraud by hour analysis complete.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6905fb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: Section 5 (Preprocessing) - Define Categorical Pipeline\n",
    "#\n",
    "# Define StringIndexer and OneHotEncoder for the 'type' column.\n",
    "#\n",
    "\n",
    "try:\n",
    "    log.info(\"Defining categorical preprocessing stages...\")\n",
    "    \n",
    "    # 1. StringIndexer\n",
    "    type_indexer = StringIndexer(\n",
    "        inputCol=\"type\", \n",
    "        outputCol=\"type_index\", \n",
    "        handleInvalid=\"keep\" # Keep unseen categories\n",
    "    )\n",
    "    \n",
    "    # 2. OneHotEncoder\n",
    "    type_encoder = OneHotEncoder(\n",
    "        inputCol=\"type_index\", \n",
    "        outputCol=\"type_vec\"\n",
    "    )\n",
    "    \n",
    "    log.info(\"StringIndexer and OneHotEncoder stages defined.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"Error defining categorical stages: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Cell 19: Categorical pipeline stages defined.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b6ff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20: Section 5 (Preprocessing) - Define Numerical Pipeline\n",
    "#\n",
    "# Define VectorAssembler and StandardScaler for numerical features.\n",
    "#\n",
    "\n",
    "try:\n",
    "    log.info(\"Defining numerical preprocessing stages...\")\n",
    "    \n",
    "    # 3. VectorAssembler\n",
    "    # Combine all numerical + encoded categorical features\n",
    "    \n",
    "    # Note: 'step' and 'hour_of_day' are included\n",
    "    feature_cols = [\n",
    "        \"type_vec\", # From OHE\n",
    "        \"step\",\n",
    "        \"amount\",\n",
    "        \"oldbalanceOrg\",\n",
    "        \"newbalanceOrig\",\n",
    "        \"oldbalanceDest\",\n",
    "        \"newbalanceDest\",\n",
    "        \"hour_of_day\"\n",
    "    ]\n",
    "    \n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=feature_cols,\n",
    "        outputCol=\"assembled_features\",\n",
    "        handleInvalid=\"skip\" # Skip rows with nulls (though we have none)\n",
    "    )\n",
    "    \n",
    "    # 4. StandardScaler\n",
    "    scaler = StandardScaler(\n",
    "        inputCol=\"assembled_features\",\n",
    "        outputCol=\"features\", # This is the final column models will use\n",
    "        withStd=True,\n",
    "        withMean=True\n",
    "    )\n",
    "    \n",
    "    log.info(\"VectorAssembler and StandardScaler stages defined.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"Error defining assembly scaling stages: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Cell 20: Feature vector assembly and scaling stages defined.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13ce7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21: Section 5 (Preprocessing) - Create Full Preprocessing Pipeline\n",
    "#\n",
    "# Combine all preprocessing stages into one pipeline.\n",
    "# We will fit this pipeline on the *entire* dataset to learn\n",
    "# the StringIndexer mapping and scaling statistics.\n",
    "#\n",
    "\n",
    "try:\n",
    "    log.info(\"Combining all preprocessing stages into a single pipeline...\")\n",
    "    \n",
    "    # Define the preprocessing pipeline\n",
    "    preproc_pipeline = Pipeline(\n",
    "        stages=[type_indexer, type_encoder, assembler, scaler]\n",
    "    )\n",
    "    \n",
    "    log.info(\"Fitting the preprocessing pipeline on the full dataset...\")\n",
    "    # This learns all indexer maps, OHE, and scaling (mean/std)\n",
    "    preproc_model = preproc_pipeline.fit(df_with_velocity)\n",
    "    \n",
    "    log.info(\"Transforming the full dataset with the preprocessing pipeline...\")\n",
    "    # This creates the final 'features' and 'label' columns\n",
    "    df_processed = preproc_model.transform(df_with_velocity).select(\n",
    "        \"features\", \n",
    "        col(\"isFraud\").alias(\"label\") # Rename for ML\n",
    "    )\n",
    "    \n",
    "    # --- *** MODIFICATION: Write to HDFS instead of persisting *** ---\n",
    "    log.info(f\"Writing fully preprocessed data to: {HDFS_PROCESSED_PATH}\")\n",
    "    df_processed.write.mode(\"overwrite\").parquet(HDFS_PROCESSED_PATH)\n",
    "    \n",
    "    log.info(\"Dataset fully preprocessed and written to HDFS.\")\n",
    "    # We can't show() here as it would require re-reading, we'll see it in the next cell\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"Error fitting transforming with preprocessing pipeline: {e}\")\n",
    "    raise e\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Cell 21: Full dataset preprocessed and written to HDFS.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5dbb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 22: Section 5 (Preprocessing) - Train/Test Split\n",
    "#\n",
    "# Split the preprocessed data into training and testing sets.\n",
    "#\n",
    "\n",
    "try:\n",
    "    log.info(\"Reading preprocessed data from HDFS for splitting...\")\n",
    "    # --- *** NEW: Read the processed data back *** ---\n",
    "    df_processed = spark.read.parquet(HDFS_PROCESSED_PATH)\n",
    "    \n",
    "    log.info(f\"Splitting data into 80% train and 20% test...\")\n",
    "    df_processed.show(5, truncate=False) # Show sample now\n",
    "    \n",
    "    # We split the *unbalanced* data to get a realistic test set\n",
    "    (unbalanced_train_df, test_df) = df_processed.randomSplit([0.8, 0.2], seed=42)\n",
    "    \n",
    "    # --- *** MODIFICATION: Write splits to HDFS *** ---\n",
    "    log.info(f\"Writing test set to: {HDFS_TEST_PATH}\")\n",
    "    test_df.write.mode(\"overwrite\").parquet(HDFS_TEST_PATH)\n",
    "    \n",
    "    log.info(f\"Writing unbalanced train set to: {HDFS_UNBALANCED_TRAIN_PATH}\")\n",
    "    unbalanced_train_df.write.mode(\"overwrite\").parquet(HDFS_UNBALANCED_TRAIN_PATH)\n",
    "    \n",
    "    # Log counts (this triggers the write actions)\n",
    "    log.info(f\"Unbalanced Train Set Records: {unbalanced_train_df.count()}\")\n",
    "    log.info(f\"Test Set Records: {test_df.count()}\")\n",
    "    \n",
    "    log.info(\"Test set class distribution:\")\n",
    "    test_df.groupBy(\"label\").count().show()\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"Error splitting data: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Cell 22: Train/Test split complete and written to HDFS.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9e0f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 23: Section 5 (Preprocessing) - Handle Class Imbalance (Oversampling)\n",
    "#\n",
    "# We apply oversampling *only* to the training set.\n",
    "#\n",
    "\n",
    "try:\n",
    "    log.info(\"Handling class imbalance on training data using oversampling...\")\n",
    "    \n",
    "    # --- *** NEW: Read the unbalanced train data *** ---\n",
    "    log.info(f\"Reading unbalanced train data from: {HDFS_UNBALANCED_TRAIN_PATH}\")\n",
    "    unbalanced_train_df = spark.read.parquet(HDFS_UNBALANCED_TRAIN_PATH)\n",
    "    \n",
    "    # Separate the training set by class\n",
    "    df_train_normal = unbalanced_train_df.filter(col(\"label\") == 0)\n",
    "    df_train_fraud = unbalanced_train_df.filter(col(\"label\") == 1)\n",
    "    \n",
    "    # Get the counts\n",
    "    count_normal = df_train_normal.count()\n",
    "    count_fraud = df_train_fraud.count()\n",
    "    \n",
    "    if count_fraud == 0:\n",
    "        log.error(\"No fraud data in training set. Cannot oversample.\")\n",
    "        raise ValueError(\"No fraud data in training set.\")\n",
    "    \n",
    "    # Calculate the ratio for oversampling\n",
    "    oversample_ratio = int(count_normal / count_fraud)\n",
    "    log.info(f\"Oversampling fraud data by a ratio of {oversample_ratio}...\")\n",
    "\n",
    "    # Oversample the fraud data\n",
    "    df_train_fraud_oversampled = df_train_fraud.sample(\n",
    "        withReplacement=True, \n",
    "        fraction=float(oversample_ratio), \n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # Combine the original normal data with the oversampled fraud data\n",
    "    train_df = df_train_normal.unionAll(df_train_fraud_oversampled)\n",
    "    \n",
    "    # --- *** MODIFICATION: Write balanced train set to HDFS *** ---\n",
    "    log.info(f\"Writing balanced train set to: {HDFS_TRAIN_PATH}\")\n",
    "    train_df.write.mode(\"overwrite\").parquet(HDFS_TRAIN_PATH)\n",
    "    \n",
    "    log.info(\"Oversampling complete. Final balanced training set stats:\")\n",
    "    log.info(f\"Total balanced train records: {train_df.count()}\")\n",
    "    train_df.groupBy(\"label\").count().show()\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"Error handling class imbalance: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Cell 23: Class imbalance handled and written to HDFS.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618f3d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 24: Section 6 (Model Training) - Random Forest (RF)\n",
    "#\n",
    "# Train the Random Forest Classifier.\n",
    "#\n",
    "\n",
    "try:\n",
    "    # --- *** NEW: Read train and test data *** ---\n",
    "    log.info(f\"Reading balanced train data from: {HDFS_TRAIN_PATH}\")\n",
    "    train_df = spark.read.parquet(HDFS_TRAIN_PATH)\n",
    "    log.info(f\"Reading test data from: {HDFS_TEST_PATH}\")\n",
    "    test_df = spark.read.parquet(HDFS_TEST_PATH)\n",
    "    \n",
    "    log.info(\"Training Random Forest model...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Define the classifier\n",
    "    rf = RandomForestClassifier(\n",
    "        labelCol=\"label\",\n",
    "        featuresCol=\"features\",\n",
    "        numTrees=100,\n",
    "        maxDepth=20, # Deep trees to capture complex patterns\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # Train the model on the balanced training data\n",
    "    rf_model = rf.fit(train_df)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    rf_train_time = end_time - start_time\n",
    "    \n",
    "    log.info(f\"Random Forest training complete. Time taken: {rf_train_time:.2f} seconds.\")\n",
    "\n",
    "    # Get predictions for the test set\n",
    "    rf_predictions = rf_model.transform(test_df)\n",
    "    \n",
    "    # --- *** MODIFICATION: Removed persist() *** ---\n",
    "    # rf_predictions.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "    \n",
    "    log.info(\"Random Forest predictions generated for test set.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"Error training Random Forest: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"Cell 24: Random Forest model trained in {rf_train_time:.2f}s.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6783f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 25: Section 6 (Model Training) - Gradient-Boosted Trees (GBT)\n",
    "#\n",
    "# Train the GBT Classifier.\n",
    "#\n",
    "\n",
    "try:\n",
    "    # --- *** NEW: Read train and test data *** ---\n",
    "    log.info(f\"Reading balanced train data from: {HDFS_TRAIN_PATH}\")\n",
    "    train_df = spark.read.parquet(HDFS_TRAIN_PATH)\n",
    "    log.info(f\"Reading test data from: {HDFS_TEST_PATH}\")\n",
    "    test_df = spark.read.parquet(HDFS_TEST_PATH)\n",
    "    \n",
    "    log.info(\"Training Gradient-Boosted Trees (GBT) model...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Define the classifier\n",
    "    gbt = GBTClassifier(\n",
    "        labelCol=\"label\",\n",
    "        featuresCol=\"features\",\n",
    "        maxIter=100, # 100 iterations\n",
    "        maxDepth=10, # Deeper than default (5)\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # Train the model on the balanced training data\n",
    "    gbt_model = gbt.fit(train_df)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    gbt_train_time = end_time - start_time\n",
    "    \n",
    "    log.info(f\"GBT training complete. Time taken: {gbt_train_time:.2f} seconds.\")\n",
    "\n",
    "    # Get predictions for the test set\n",
    "    gbt_predictions = gbt_model.transform(test_df)\n",
    "    \n",
    "    # --- *** MODIFICATION: Removed persist() *** ---\n",
    "    # gbt_predictions.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "    \n",
    "    log.info(\"GBT predictions generated for test set.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"Error training GBT: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"Cell 25: GBT model trained in {gbt_train_time:.2f}s.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9451c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 26: Section 6 (Model Training) - Isolation Forest (IF)\n",
    "#\n",
    "# Train an unsupervised Isolation Forest for anomaly detection.\n",
    "# Note: This model is trained on UNLABELED data.\n",
    "#\n",
    "\n",
    "try:\n",
    "    # --- *** NEW: Read UNBALANCED train and test data *** ---\n",
    "    log.info(f\"Reading unbalanced train data from: {HDFS_UNBALANCED_TRAIN_PATH}\")\n",
    "    unbalanced_train_df = spark.read.parquet(HDFS_UNBALANCED_TRAIN_PATH)\n",
    "    log.info(f\"Reading test data from: {HDFS_TEST_PATH}\")\n",
    "    test_df = spark.read.parquet(HDFS_TEST_PATH)\n",
    "    \n",
    "    log.info(\"Training Isolation Forest (IF) model...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Define the anomaly detector. It does not use the 'label' col.\n",
    "    # We train this on the *unbalanced* training set.\n",
    "    iso_forest = IsolationForest(\n",
    "        featuresCol=\"features\",\n",
    "        seed=42,\n",
    "        contamination=0.001 # Set to expected fraud rate\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    if_model = iso_forest.fit(unbalanced_train_df)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    if_train_time = end_time - start_time\n",
    "    \n",
    "    log.info(f\"Isolation Forest training complete. Time taken: {if_train_time:.2f} seconds.\\\")\n",
    "\n",
    "    # Get predictions for the test set\n",
    "    if_predictions = if_model.transform(test_df)\n",
    "    \n",
    "    # --- *** MODIFICATION: Removed persist() *** ---\n",
    "    # if_predictions.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "    \n",
    "    log.info(\\\"Isolation Forest predictions generated for test set.\\\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\\\"Error training Isolation Forest: {e}\\\")\n",
    "\n",
    "print(\\\"=\\\"*80)\n",
    "print(f\\\"Cell 26: Isolation Forest model trained in {if_train_time:.2f}s.\\\")\n",
    "print(\\\"=\\\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3260928f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 27: Section 6 - Model: Feature Importance\n",
    "#\n",
    "# Extract and display feature importances from tree-based models.\n",
    "#\n",
    "\n",
    "def get_feature_importances(pipeline_model, model):\n",
    "    \"\"\"\n",
    "    Helper function to map feature importances back to original column names.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get the feature names from the VectorAssembler stage\n",
    "        assembler = pipeline_model.stages[2] # 0=Indexer, 1=Encoder, 2=Assembler\n",
    "        \n",
    "        # Get OHE metadata\n",
    "        ohe_output_col = pipeline_model.stages[1].getOutputCol()\n",
    "        ohe_attrs = pipeline_model.stages[0].transform(spark.createDataFrame([(\"\",)], [\"type\"])).schema[ohe_output_col].metadata\n",
    "        ohe_labels = ohe_attrs[\"ml_attr\"][\"attrs\"][\"nominal\"][\"values\"]\n",
    "\n",
    "        feature_names = []\n",
    "        for f in assembler.getInputCols():\n",
    "            if f == ohe_output_col:\n",
    "                feature_names.extend([f\"type_{label}\" for label in ohe_labels])\n",
    "            else:\n",
    "                feature_names.append(f)\n",
    "        \n",
    "        if not feature_names:\n",
    "            log.warning(\"Could not extract feature names.\")\n",
    "            return None\n",
    "        \n",
    "        # Get importances\n",
    "        importances = model.featureImportances.toArray()\n",
    "        \n",
    "        # Create a Pandas DataFrame\n",
    "        df_importance = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importances\n",
    "        }).sort_values(by='importance', ascending=False)\n",
    "        \n",
    "        return df_importance\n",
    "\n",
    "    except Exception as e:\n",
    "        log.error(f\"Error getting feature importances: {e}\")\n",
    "        return None\n",
    "\n",
    "try:\n",
    "    log.info(\"Extracting Random Forest Feature Importances...\")\n",
    "    # rf_model is just the model, not the pipeline. We need the preproc_model\n",
    "    # We must re-read the preproc_model from Cell 21 (now 20)\n",
    "    # Wait, it's not saved. We need to re-fit it.\n",
    "    #\n",
    "    # *** MODIFICATION: This cell is problematic as it assumes preproc_model\n",
    "    # is in memory. We will re-load df_with_velocity and re-fit the pipeline\n",
    "    # just to get the feature names.\n",
    "    # This is slow, but required by our \"no-persist\" strategy.\n",
    "    \n",
    "    log.info(\"Re-fitting preproc pipeline to get feature names...\")\n",
    "    temp_df = spark.read.parquet(HDFS_DATA_PATH).drop('nameOrig', 'nameDest', 'isFlaggedFraud')\n",
    "    temp_df_with_time = temp_df.withColumn(\"hour_of_day\", (col(\"step\") - 1) % 24)\n",
    "    preproc_pipeline = Pipeline(stages=[type_indexer, type_encoder, assembler, scaler])\n",
    "    preproc_model_for_features = preproc_pipeline.fit(temp_df_with_time)\n",
    "    \n",
    "    \n",
    "    rf_importance_pd = get_feature_importances(preproc_model_for_features, rf_model)\n",
    "    if rf_importance_pd is not None:\n",
    "        print(\"\\n--- Random Forest Top 10 Features ---\")\n",
    "        print(rf_importance_pd.head(10))\n",
    "\n",
    "    log.info(\"Extracting GBT Feature Importances...\")\n",
    "    gbt_importance_pd = get_feature_importances(preproc_model_for_features, gbt_model)\n",
    "    if gbt_importance_pd is not None:\n",
    "        print(\"\\n--- GBT Top 10 Features ---\")\n",
    "        print(gbt_importance_pd.head(10))\n",
    "    \n",
    "except Exception as e:\n",
    "    log.error(f\"Error displaying feature importances: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Cell 27: Feature importances extracted.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2900ce71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 28: Section 7 (Evaluation) - Metrics Helper Function\n",
    "#\n",
    "# Create a reusable function to calculate all required metrics.\n",
    "#\n",
    "\n",
    "def evaluate_model(predictions, model_name):\n",
    "    \"\"\"\n",
    "    Calculates and prints a full suite of metrics for a binary classifier's predictions.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        log.info(f\"--- Evaluating Model: {model_name} ---\")\n",
    "        \n",
    "        # --- Standard Metrics (Accuracy, F1, Precision, Recall) ---\n",
    "        # Note: 'weightedPrecision'/'weightedRecall' are just Precision/Recall for binary\n",
    "        eval_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "        eval_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "        eval_prec = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "        eval_rec = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "        \n",
    "        accuracy = eval_acc.evaluate(predictions)\n",
    "        f1_score = eval_f1.evaluate(predictions)\n",
    "        precision = eval_prec.evaluate(predictions)\n",
    "        recall = eval_rec.evaluate(predictions)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1-Score: {f1_score:.4f}\")\n",
    "\n",
    "        # --- AUC-ROC and AUC-PR (Area Under Curve) ---\n",
    "        # These require the 'rawPrediction' or 'probability' column\n",
    "        prob_col = \"rawPrediction\"\n",
    "        if \"probability\" in predictions.columns:\n",
    "            prob_col = \"probability\"\n",
    "        \n",
    "        eval_roc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=prob_col, metricName=\"areaUnderROC\")\n",
    "        eval_pr = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=prob_col, metricName=\"areaUnderPR\")\n",
    "        \n",
    "        roc_auc = eval_roc.evaluate(predictions)\n",
    "        pr_auc = eval_pr.evaluate(predictions)\n",
    "        \n",
    "        print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "        print(f\"PR-AUC: {pr_auc:.4f}\")\n",
    "        \n",
    "        # --- Confusion Matrix ---\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        predictions.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\").show()\n",
    "        \n",
    "        # Return metrics for summary table\n",
    "        metrics = {\n",
    "            \"Model\": model_name,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1-Score\": f1_score,\n",
    "            \"ROC-AUC\": roc_auc,\n",
    "            \"PR-AUC\": pr_auc,\n",
    "            \"Train Time (s)\": 0 # Placeholder\n",
    "        }\n",
    "        return metrics, predictions\n",
    "\n",
    "    except Exception as e:\n",
    "        log.error(f\"Error evaluating model {model_name}: {e}\")\n",
    "        # Check if 'rawPrediction' or 'probability' exists\n",
    "        if \"rawPrediction\" not in predictions.columns and \"probability\" not in predictions.columns:\n",
    "            log.error(\"Evaluation failed. 'rawPrediction' or 'probability' column not found.\")\n",
    "        return None, None\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Cell 28: Evaluation helper function defined.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce76ab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 29: Section 7 (Evaluation) - Calculate Metrics for All Models\n",
    "\n",
    "try:\n",
    "    all_metrics = []\n",
    "    \n",
    "    # 1. Evaluate Random Forest\n",
    "    # rf_predictions will be re-computed here\n",
    "    rf_metrics, rf_predictions = evaluate_model(rf_predictions, \"Random Forest\")\n",
    "    rf_metrics[\"Train Time (s)\"] = rf_train_time\n",
    "    all_metrics.append(rf_metrics)\n",
    "\n",
    "    # 2. Evaluate GBT\n",
    "    # gbt_predictions will be re-computed here\n",
    "    gbt_metrics, gbt_predictions = evaluate_model(gbt_predictions, \"GBT\")\n",
    "    gbt_metrics[\"Train Time (s)\"] = gbt_train_time\n",
    "    all_metrics.append(gbt_metrics)\n",
    "    \n",
    "    # 3. Evaluate Isolation Forest\n",
    "    # if_predictions will be re-computed here\n",
    "    log.info(\"--- Evaluating Model: Isolation Forest ---\")\n",
    "    eval_acc_if = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    eval_f1_if = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "    eval_prec_if = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "    eval_rec_if = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "    \n",
    "    if_accuracy = eval_acc_if.evaluate(if_predictions)\n",
    "    if_f1_score = eval_f1_if.evaluate(if_predictions)\n",
    "    if_precision = eval_prec_if.evaluate(if_predictions)\n",
    "    if_recall = eval_rec_if.evaluate(if_predictions)\n",
    "    \n",
    "    print(f\"Accuracy: {if_accuracy:.4f}\")\n",
    "    print(f\"Precision: {if_precision:.4f}\")\n",
    "    print(f\"Recall: {if_recall:.4f}\")\n",
    "    print(f\"F1-Score: {if_f1_score:.4f}\")\n",
    "    print(\"ROC-AUC: N/A\")\n",
    "    print(\"PR-AUC: N/A\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    if_predictions.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\").show()\n",
    "    \n",
    "    if_metrics = {\n",
    "        \"Model\": \"Isolation Forest\",\n",
    "        \"Accuracy\": if_accuracy,\n",
    "        \"Precision\": if_precision,\n",
    "        \"Recall\": if_recall,\n",
    "        \"F1-Score\": if_f1_score,\n",
    "        \"ROC-AUC\": None,\n",
    "        \"PR-AUC\": None,\n",
    "        \"Train Time (s)\": if_train_time\n",
    "    }\n",
    "    all_metrics.append(if_metrics)\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"Error during bulk model evaluation: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Cell 29: All models evaluated.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60d0227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 30: Section 7 (Evaluation) - Summary Table\n",
    "\n",
    "try:\n",
    "    log.info(\"Generating model performance summary table...\")\n",
    "    \n",
    "    # Create a Pandas DataFrame from the collected metrics\n",
    "    metrics_summary_pd = pd.DataFrame(all_metrics).set_index(\"Model\")\n",
    "    \n",
    "    # Format for better readability\n",
    "    styled_summary = metrics_summary_pd.style.format({\n",
    "        \"Accuracy\": \"{:.4f}\",\n",
    "        \"Precision\": \"{:.4f}\",\n",
    "        \"Recall\": \"{:.4f}\",\n",
    "        \"F1-Score\": \"{:.4f}\",\n",
    "        \"ROC-AUC\": \"{:.4f}\",\n",
    "        \"PR-AUC\": \"{:.4f}\",\n",
    "        \"Train Time (s)\": \"{:.2f}\"\n",
    "    }).highlight_max(\n",
    "        subset=[\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\", \"ROC-AUC\", \"PR-AUC\"], \n",
    "        color='lightgreen'\n",
    "    ).highlight_min(\n",
    "        subset=[\"Train Time (s)\"], \n",
    "        color='lightyellow'\n",
    "    )\n",
    "    \n",
    "    print(\"\\n--- Model Performance Summary ---\")\n",
    "    \n",
    "    # Save to local file\n",
    "    metrics_path = os.path.join(METRICS_OUTPUT_DIR, \"model_summary.html\")\n",
    "    styled_summary.to_html(metrics_path)\n",
    "    log.info(f\"Saved metrics summary to {metrics_path}\")\n",
    "    \n",
    "    # Display in notebook\n",
    "    display(styled_summary)\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"Error generating summary table: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Cell 30: Model performance summary generated.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9ad4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 31: Section 7 (Evaluation) - Confusion Matrix (Visualization)\n",
    "#\n",
    "# Plot pretty confusion matrices for the top models.\n",
    "#\n",
    "\n",
    "def plot_confusion_matrix(predictions, model_name):\n",
    "    \"\"\"Generates and saves a confusion matrix plot.\"\"\"\n",
    "    try:\n",
    "        log.info(f\"Plotting confusion matrix for {model_name}...\")\n",
    "        \n",
    "        # Collect data for plotting\n",
    "        # This is a small aggregate, so .toPandas() is safe\n",
    "        cm_data = predictions.groupBy(\\\"label\\\", \\\"prediction\\\").count().toPandas()\n",
    "        \n",
    "        # Pivot to create the matrix\n",
    "        cm_matrix = cm_data.pivot(\n",
    "            index='label', \n",
    "            columns='prediction', \n",
    "            values='count'\n",
    "        ).fillna(0)\n",
    "        \n",
    "        # Ensure we have a 2x2 matrix even if one class wasn't predicted\n",
    "        if 0 not in cm_matrix.index: cm_matrix.loc[0] = 0\n",
    "        if 1 not in cm_matrix.index: cm_matrix.loc[1] = 0\n",
    "        if 0.0 not in cm_matrix.columns: cm_matrix[0.0] = 0\n",
    "        if 1.0 not in cm_matrix.columns: cm_matrix[1.0] = 0\n",
    "        \n",
    "        # Sort index and columns to ensure correct TN/FP/FN/TP placement\n",
    "        cm_matrix = cm_matrix.sort_index(axis=0)[sorted(cm_matrix.columns)]\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(\n",
    "            cm_matrix, \n",
    "            annot=True, \n",
    "            fmt='g', \n",
    "            cmap='Blues', \n",
    "            xticklabels=['Normal (0)', 'Fraud (1)'],\n",
    "            yticklabels=['Normal (0)', 'Fraud (1)']\n",
    "        )\n",
    "        plt.title(f'{model_name} - Confusion Matrix', fontsize=16)\n",
    "        plt.xlabel('Predicted Label', fontsize=12)\n",
    "        plt.ylabel('True Label', fontsize=12)\n",
    "        \n",
    "        plot_path = os.path.join(PLOT_OUTPUT_DIR, f'06_cm_{model_name.replace(\" \", \"_\")}.png')\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        log.info(f\"Saved {model_name} confusion matrix to {plot_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        log.error(f\"Error plotting confusion matrix: {e}\")\n",
    "\n",
    "# Plot for Random Forest and GBT\n",
    "plot_confusion_matrix(rf_predictions, \"Random Forest\")\n",
    "plot_confusion_matrix(gbt_predictions, \"GBT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb182838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 32: Section 7 (Evaluation) - ROC and PR Curves\n",
    "#\n",
    "# This requires collecting probabilities. We will sample the test set\n",
    "# to avoid OOM errors on the driver.\n",
    "#\n",
    "\n",
    "def plot_roc_pr_curves(predictions_list, model_names):\n",
    "    \"\"\"\n",
    "    Plots ROC and PR curves for a list of models on one chart.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        log.info(\"Plotting ROC and PR curves...\")\n",
    "        \n",
    "        plt.figure(figsize=(20, 8))\n",
    "        \n",
    "        # --- 1. ROC Curve Plot ---\n",
    "        plt.subplot(1, 2, 1)\n",
    "        \n",
    "        for i, predictions in enumerate(predictions_list):\n",
    "            model_name = model_names[i]\n",
    "            \n",
    "            # Extract probability for class 1\n",
    "            # We must sample to avoid OOM\n",
    "            preds_sample_pd = predictions.select(\n",
    "                \"label\", \n",
    "                \"probability\"\n",
    "            ).sample(0.1, seed=42).toPandas()\n",
    "            \n",
    "            # Extract prob of \"1\" (fraud)\n",
    "            y_prob = [p[1] for p in preds_sample_pd['probability']]\n",
    "            y_true = preds_sample_pd['label']\n",
    "            \n",
    "            fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "            roc_auc_val = auc(fpr, tpr)\n",
    "            \n",
    "            plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {roc_auc_val:.4f})')\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate', fontsize=12)\n",
    "        plt.ylabel('True Positive Rate', fontsize=12)\n",
    "        plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=16)\n",
    "        plt.legend(loc=\"lower right\")\n",
    "\n",
    "        # --- 2. PR Curve Plot ---\n",
    "        plt.subplot(1, 2, 2)\n",
    "        \n",
    "        for i, predictions in enumerate(predictions_list):\n",
    "            model_name = model_names[i]\n",
    "            \n",
    "            preds_sample_pd = predictions.select(\n",
    "                \"label\", \n",
    "                \"probability\"\n",
    "            ).sample(0.1, seed=42).toPandas()\n",
    "            \n",
    "            y_prob = [p[1] for p in preds_sample_pd['probability']]\n",
    "            y_true = preds_sample_pd['label']\n",
    "            \n",
    "            precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "            pr_auc_val = average_precision_score(y_true, y_prob)\n",
    "            \n",
    "            plt.plot(recall, precision, lw=2, label=f'{model_name} (AUC = {pr_auc_val:.4f})')\n",
    "        \n",
    "        plt.xlabel('Recall', fontsize=12)\n",
    "        plt.ylabel('Precision', fontsize=12)\n",
    "        plt.title('Precision-Recall (PR) Curve', fontsize=16)\n",
    "        plt.legend(loc=\"lower left\")\n",
    "\n",
    "        # --- Save and Show ---\n",
    "        plot_path = os.path.join(PLOT_OUTPUT_DIR, '07_roc_pr_curves.png')\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        log.info(f\"Saved ROC and PR curve plot to {plot_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        log.error(f\"Error plotting ROC/PR curves: {e}\")\n",
    "\n",
    "# Plot for RF and GBT (IF doesn't have probabilities)\n",
    "plot_roc_pr_curves([rf_predictions, gbt_predictions], [\"Random Forest\", \"GBT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e118aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 33: Section 8 (Advanced) - Identify Repeat Fraudsters\n",
    "#\n",
    "# Use Hive SQL to find accounts ('nameOrig') that have\n",
    "# committed more than one fraudulent transaction.\n",
    "#\n",
    "\n",
    "try:\n",
    "    log.info(\"Analyzing for repeat fraudulent senders...\")\n",
    "    \n",
    "    sql_query = f\"\"\"\n",
    "    SELECT \n",
    "        nameOrig, \n",
    "        SUM(isFraud) as total_fraud_transactions,\n",
    "        SUM(amount) as total_fraud_amount,\n",
    "        COUNT(*) as total_transactions\n",
    "    FROM {HIVE_TABLE_NAME}\n",
    "    WHERE isFraud = 1\n",
    "    GROUP BY nameOrig\n",
    "    HAVING total_fraud_transactions > 1\n",
    "    ORDER BY total_fraud_transactions DESC, total_fraud_amount DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    repeat_fraudsters_df = spark.sql(sql_query)\n",
    "    \n",
    "    log.info(\"Top Repeat Fraudsters (Senders):\")\n",
    "    repeat_fraudsters_df.show()\n",
    "    \n",
    "    log.info(f\"Found {repeat_fraudsters_df.count()} accounts that sent > 1 fraudulent transaction.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    log.error(f\"Error identifying repeat fraudsters: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Cell 33: Repeat fraudster analysis complete.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67a43f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 34: Section 8 (Advanced) - Fraud by Amount Ranges\n",
    "\n",
    "try:\n",
    "    log.info(\"Analyzing fraud patterns by transaction amount range...\")\n",
    "    \n",
    "    sql_query = f\"\"\"\n",
    "    SELECT \n",
    "        CASE\n",
    "            WHEN amount < 1000 THEN '1. < 1K'\n",
    "            WHEN amount BETWEEN 1000 AND 10000 THEN '2. 1K - 10K'\n",
    "            WHEN amount BETWEEN 10000 AND 100000 THEN '3. 10K - 100K'\n",
    "            WHEN amount BETWEEN 100000 AND 1000000 THEN '4. 100K - 1M'\n",
    "            ELSE '5. > 1M'\n",
    "        END as amount_range,\n",
    "        SUM(isFraud) as total_fraud,\n",
    "        COUNT(*) as total_transactions,\n",
    "        (SUM(isFraud) / COUNT(*)) * 100 as fraud_rate_in_range\n",
    "    FROM {HIVE_TABLE_NAME}\n",
    "    GROUP BY amount_range\n",
    "    ORDER BY amount_range\n",
    "    \"\"\"\n",
    "    \n",
    "    amount_range_df = spark.sql(sql_query)\n",
    "    \n",
    "    log.info(\"Fraud Statistics by Amount Range:\")\n",
    "    amount_range_df.show()\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"Error analyzing fraud by amount ranges: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Cell 34: Fraud by amount range analysis complete.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893f527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 35: Section 9 (Time-Series) - Hourly Fraud Heatmap\n",
    "#\n",
    "# Use a SQL query to create a heatmap of transactions\n",
    "# by 'hour_of_day' and 'type'.\n",
    "#\n",
    "\n",
    "try:\n",
    "    log.info(\"Generating hourly transaction heatmap data...\")\n",
    "    \n",
    "    # We use the 'transactions_with_time' view from Cell 18\n",
    "    sql_query = f\"\"\"\n",
    "    SELECT \n",
    "        hour_of_day,\n",
    "        type,\n",
    "        COUNT(*) as num_transactions,\n",
    "        SUM(isFraud) as num_fraud\n",
    "    FROM transactions_with_time\n",
    "    GROUP BY hour_of_day, type\n",
    "    \"\"\"\n",
    "    \n",
    "    heatmap_df = spark.sql(sql_query)\n",
    "    heatmap_pd = heatmap_df.toPandas()\n",
    "    \n",
    "    # --- Plot 1: Total Transactions ---\n",
    "    heatmap_total_pd = heatmap_pd.pivot(\n",
    "        index=\"type\", \n",
    "        columns=\"hour_of_day\", \n",
    "        values=\"num_transactions\"\n",
    "    ).fillna(0)\n",
    "    \n",
    "    plt.figure(figsize=(20, 4))\n",
    "    sns.heatmap(heatmap_total_pd, annot=True, fmt='g', cmap='viridis')\n",
    "    plt.title('Heatmap of Total Transactions by Hour and Type', fontsize=16)\n",
    "    plt.xlabel('Hour of Day', fontsize=12)\n",
    "    plt.ylabel('Transaction Type', fontsize=12)\n",
    "    \n",
    "    plot_path = os.path.join(PLOT_OUTPUT_DIR, '08_heatmap_total.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    log.info(f\"Saved total transactions heatmap to {plot_path}\")\n",
    "    plt.show()\n",
    "\n",
    "    # --- Plot 2: Fraud Transactions ---\n",
    "    heatmap_fraud_pd = heatmap_pd.pivot(\n",
    "        index=\"type\", \n",
    "        columns=\"hour_of_day\", \n",
    "        values=\"num_fraud\"\n",
    "    ).fillna(0)\n",
    "    \n",
    "    plt.figure(figsize=(20, 4))\n",
    "    sns.heatmap(heatmap_fraud_pd, annot=True, fmt='g', cmap='Reds')\n",
    "    plt.title('Heatmap of FRAUD Transactions by Hour and Type', fontsize=16)\n",
    "    plt.xlabel('Hour of Day', fontsize=12)\n",
    "    plt.ylabel('Transaction Type', fontsize=12)\n",
    "    \n",
    "    plot_path = os.path.join(PLOT_OUTPUT_DIR, '09_heatmap_fraud.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    log.info(f\"Saved fraud transactions heatmap to {plot_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    log.error(f\"Error generating transaction heatmaps: {e}\\\")\n",
    "\n",
    "print(\\\"=\\\"*80)\n",
    "print(\"Cell 35: Transaction heatmap analysis complete.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c182188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 36: Section 9 (Model Saving) - Save Best Model\n",
    "#\n",
    "# Save the Preprocessing Pipeline and the GBT Model.\n",
    "# GBT is often a good balance of performance and speed.\n",
    "#\n",
    "\n",
    "try:\n",
    "    log.info(\"Saving preprocessing pipeline and GBT model to local filesystem...\")\n",
    "    \n",
    "    # --- 1. Save Preprocessing Pipeline ---\n",
    "    # We must re-fit it one last time as it's not in memory\n",
    "    log.info(\"Re-fitting and saving preprocessing pipeline...\")\n",
    "    \n",
    "    # Reload the data it was trained on\n",
    "    temp_df = spark.read.parquet(HDFS_DATA_PATH).drop('nameOrig', 'nameDest', 'isFlaggedFraud')\n",
    "    temp_df_with_time = temp_df.withColumn(\"hour_of_day\", (col(\"step\") - 1) % 24)\n",
    "    \n",
    "    # Define pipeline\n",
    "    preproc_pipeline = Pipeline(\n",
    "        stages=[type_indexer, type_encoder, assembler, scaler]\n",
    "    )\n",
    "    # Fit\n",
    "    preproc_model_to_save = preproc_pipeline.fit(temp_df_with_time)\n",
    "    \n",
    "    # Save\n",
    "    preproc_path = os.path.join(MODEL_OUTPUT_DIR, \"fraud_preproc_pipeline_v1\")\n",
    "    preproc_model_to_save.write().overwrite().save(preproc_path)\n",
    "    \n",
    "    log.info(f\"Successfully saved preprocessing pipeline to: {preproc_path}\")\n",
    "\n",
    "    # --- 2. Save GBT Model ---\n",
    "    # The 'gbt_model' object is in memory from Cell 25 (new)\n",
    "    log.info(\"Saving GBT model...\")\n",
    "    model_path = os.path.join(MODEL_OUTPUT_DIR, \"gbt_fraud_model_v1\")\n",
    "    gbt_model.write().overwrite().save(model_path)\n",
    "    \n",
    "    log.info(f\"Successfully saved GBT model to: {model_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"Error saving models: {e}\\\")\n",
    "\n",
    "print(\\\"=\\\"*80)\n",
    "print(\"Cell 36: Preprocessing pipeline and GBT model saved.\")\n",
    "print(\\\"=\\\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1671f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 37: Section 9 (Model Loading/Prediction) - Demo\n",
    "#\n",
    "# Demonstrate how to load the models and make a prediction.\n",
    "#\n",
    "\n",
    "try:\n",
    "    log.info(\"--- Single Prediction Demo ---\")\n",
    "    log.info(\"Loading 1 fraudulent and 1 normal transaction from test set...\")\n",
    "    \n",
    "    # --- 1. Load Models ---\n",
    "    preproc_path = os.path.join(MODEL_OUTPUT_DIR, \"fraud_preproc_pipeline_v1\")\n",
    "    model_path = os.path.join(MODEL_OUTPUT_DIR, \"gbt_fraud_model_v1\")\n",
    "    \n",
    "    preproc_model = PipelineModel.load(preproc_path)\n",
    "    model = gbt_model.load(model_path) # GBTModel\n",
    "    \n",
    "    # --- 2. Load Sample Data ---\n",
    "    # We'll re-read the test set and grab samples\n",
    "    test_df = spark.read.parquet(HDFS_TEST_PATH)\n",
    "    \n",
    "    sample_fraud = test_df.filter(\"label = 1\").limit(1)\n",
    "    sample_normal = test_df.filter(\"label = 0\").limit(1)\n",
    "    \n",
    "    sample_to_predict = sample_fraud.unionAll(sample_normal)\n",
    "    \n",
    "    log.info(\"Making predictions on sample data (already featurized)...\")\n",
    "    \n",
    "    # Note: These are already preprocessed, so we just need the model\n",
    "    final_predictions = model.transform(sample_to_predict)\n",
    "    \n",
    "    print(\"\\n--- Sample Predictions ---\")\n",
    "    final_predictions.select(\n",
    "        \"label\", \"prediction\", \"probability\"\n",
    "    ).show(truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    log.error(f\"Error in single prediction function: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Cell 37: Model loading and prediction demo complete.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d326e10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 38: Section 10 - Shutdown\n",
    "#\n",
    "# Stop the SparkSession and clean up intermediate HDFS files.\n",
    "#\n",
    "\n",
    "try:\n",
    "    log.info(\"Cleaning up and stopping SparkSession...\")\n",
    "    \n",
    "    # --- *** MODIFICATION: Remove all .unpersist() calls *** ---\n",
    "    \n",
    "    # --- *** NEW: Delete intermediate HDFS files *** ---\n",
    "    log.info(\"Deleting intermediate HDFS files...\")\n",
    "    \n",
    "    # Get the Hadoop FileSystem object\n",
    "    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "    \n",
    "    # Create Path objects\n",
    "    path_processed = spark._jvm.org.apache.hadoop.fs.Path(HDFS_PROCESSED_PATH)\n",
    "    path_unbalanced_train = spark._jvm.org.apache.hadoop.fs.Path(HDFS_UNBALANCED_TRAIN_PATH)\n",
    "    path_test = spark._jvm.org.apache.hadoop.fs.Path(HDFS_TEST_PATH)\n",
    "    path_train = spark._jvm.org.apache.hadoop.fs.Path(HDFS_TRAIN_PATH)\n",
    "    \n",
    "    # Delete them (recursive=True)\n",
    "    if fs.exists(path_processed):\n",
    "        fs.delete(path_processed, True)\n",
    "        log.info(f\"Deleted: {HDFS_PROCESSED_PATH}\")\n",
    "        \n",
    "    if fs.exists(path_unbalanced_train):\n",
    "        fs.delete(path_unbalanced_train, True)\n",
    "        log.info(f\"Deleted: {HDFS_UNBALANCED_TRAIN_PATH}\")\n",
    "        \n",
    "    if fs.exists(path_test):\n",
    "        fs.delete(path_test, True)\n",
    "        log.info(f\"Deleted: {HDFS_TEST_PATH}\")\n",
    "        \n",
    "    if fs.exists(path_train):\n",
    "        fs.delete(path_train, True)\n",
    "        log.info(f\"Deleted: {HDFS_TRAIN_PATH}\")\n",
    "    \n",
    "    log.info(\"All intermediate HDFS files deleted.\")\n",
    "    \n",
    "    # Stop the Spark context\n",
    "    spark.stop()\n",
    "    \n",
    "    log.info(\"SparkSession stopped. Notebook execution complete.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log.error(f\"Error during Spark shutdown: {e}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Cell 38: SparkSession stopped. Project complete.\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
