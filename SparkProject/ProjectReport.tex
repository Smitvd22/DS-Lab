\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}
\usepackage{url}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{tocloft}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{array}
\usepackage{multirow}
\usepackage{longtable}

% Page geometry
\geometry{
    left=2.5cm,
    right=2.5cm,
    top=2.5cm,
    bottom=2.5cm
}

% Header and footer
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Big Data Analytics Project}
\fancyhead[R]{U23AI118}
\fancyfoot[C]{\thepage}

% Title formatting
\titleformat{\section}{\Large\bfseries}{\thesection.}{1em}{}
\titleformat{\subsection}{\large\bfseries}{\thesubsection.}{1em}{}
\titleformat{\subsubsection}{\normalsize\bfseries}{\thesubsubsection.}{1em}{}

% Code listing style
\lstset{
    backgroundcolor=\color{gray!10},
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny,
    showstringspaces=false,
    tabsize=4
}

% Hyperlink setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Financial Fraud Detection using Distributed Machine Learning},
    pdfauthor={Smit Deoghare},
    pdfsubject={Big Data Analytics Project}
}

\begin{document}

% Title Page
\begin{titlepage}
    \centering
    \vspace*{3cm}
    
    {\LARGE \textbf{Big Data Analytics Project}}\\[0.8cm]
    
    {\Huge \textbf{Financial Fraud Detection using Distributed Machine Learning}}\\[1.5cm]
    
    {\Large Student Name: \textbf{Smit Deoghare}}\\[0.3cm]
    {\Large Roll Number: \textbf{U23AI118}}\\[1cm]
    
    {\large Department of Artificial Intelligence}\\[0.3cm]
    
    \vfill
\end{titlepage}

% Remove separate TOC and LOF pages for brevity

\section{Executive Summary}

This project implements an end-to-end machine learning pipeline for detecting fraudulent financial transactions using the PaySim dataset containing 6.3+ million transaction records. The key challenge is extreme class imbalance (99.87\% normal vs 0.13\% fraud transactions) requiring distributed processing capabilities.

\textbf{Solution:} A comprehensive distributed computing pipeline using Apache Spark, Hadoop, and Hive for large-scale data processing, feature engineering, and ML model training.

\textbf{Tech Stack:} Hadoop Multi-Node Cluster, Apache Spark/PySpark, Apache Hive, Jupyter Notebook, PySpark MLlib, Matplotlib/Seaborn.

\textbf{Results:} Two machine learning models successfully implemented - Random Forest (50 trees, depth 10) and Gradient Boosted Trees (GBT).

\section{System Setup and Configuration}

\subsection{Prerequisites}
\begin{itemize}
    \item \textbf{Operating System:} Ubuntu 24.04
    \item \textbf{Java:} OpenJDK 8 (openjdk version "1.8.0\_462")
    \item \textbf{Hadoop:} 2.6.5
    \item \textbf{Hive:} 1.2.2
    \item \textbf{Pig:} 0.16.0
    \item \textbf{MySQL Server:} 8.0.43
    \item \textbf{MySQL Connector JAR:} mysql-connector-java-8.0.28.jar located in /usr/local/hive/lib/
    \item \textbf{Hive JLine:} jline-2.12.jar correctly configured
    \item \textbf{Python:} 3.7
\end{itemize}

\subsection{Apache Spark Installation and Configuration}

\subsubsection{Download and Install Spark}
\begin{lstlisting}[language=bash, caption=Download Spark 2.4.8]
# Navigate to temporary directory
cd /tmp

# Download Spark binary
wget https://archive.apache.org/dist/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.6.tgz

# Extract and move to system directory
tar -xzf spark-2.4.8-bin-hadoop2.6.tgz
sudo mv spark-2.4.8-bin-hadoop2.6 /usr/local/spark

# Change ownership for current user
sudo chown -R $USER:$USER /usr/local/spark
\end{lstlisting}

\subsubsection{Environment Configuration}
\begin{lstlisting}[language=bash, caption=Configure Environment Variables]
# Edit bashrc file
nano ~/.bashrc

# Add the following lines to ~/.bashrc:
# ========= SPARK ENV VARIABLES =========
export SPARK_HOME=/usr/local/spark
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin

# Reload bashrc
source ~/.bashrc
\end{lstlisting}

\subsubsection{Spark Configuration}
\begin{lstlisting}[language=bash, caption=Configure Spark Environment]
# Navigate to Spark configuration directory
cd /usr/local/spark/conf

# Copy template and create spark-env.sh
cp spark-env.sh.template spark-env.sh
nano spark-env.sh
\end{lstlisting}

Add the following content to \texttt{spark-env.sh}:
\begin{lstlisting}[language=bash, caption=Spark Environment Configuration]
#!/usr/bin/env bash

# Set the Hadoop Configuration Directory
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop

# Add Hadoop libraries to Spark's classpath
# This ensures Spark uses your Hadoop 2.6.5 libraries
export SPARK_DIST_CLASSPATH=$(hadoop classpath)

# Explicitly set JAVA_HOME for Spark
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
\end{lstlisting}

\subsection{Hive Integration Setup}

\subsubsection{Copy Hive Configuration}
\begin{lstlisting}[language=bash, caption=Configure Hive Integration]
# Copy hive-site.xml for automatic Spark-Hive integration
cp $HIVE_HOME/conf/hive-site.xml /usr/local/spark/conf/

# Copy MySQL Connector JAR for database connectivity
cp /usr/local/hive/lib/mysql-connector-java-8.0.28.jar /usr/local/spark/jars/
\end{lstlisting}

\subsection{Python Environment Setup}

\subsubsection{Python Version Management}
\begin{lstlisting}[language=bash, caption=Install Python 3.7 (if needed)]
# Install software-properties-common for PPA management
sudo apt update
sudo apt install software-properties-common -y

# Add deadsnakes PPA for Python versions
sudo add-apt-repository ppa:deadsnakes/ppa -y
sudo apt update
sudo apt install python3.7 python3.7-venv -y
\end{lstlisting}

\subsubsection{Virtual Environment and Jupyter Setup}
\begin{lstlisting}[language=bash, caption=Create PySpark Environment]
# Create virtual environment
python3 -m venv pyspark_env

# Activate environment
source pyspark_env/bin/activate

# Install required packages
pip install jupyter notebook findspark pandas numpy matplotlib seaborn scikit-learn

# Launch Jupyter Notebook
jupyter notebook
\end{lstlisting}


\section{Dataset and Infrastructure}

\textbf{Dataset:} PaySim synthetic financial dataset (\url{https://www.kaggle.com/datasets/ealaxi/paysim1}) with 6,362,620 transaction records, 11 features, highly imbalanced (99.87\% normal, 0.13\% fraud). Key features include transaction type, amount, account balances, and fraud indicator.

\textbf{Infrastructure:} Multi-node Hadoop cluster with HDFS (replication factor 3), YARN resource management, Apache Spark (4GB driver/executor memory, Kryo serializer), and Hive integration with MySQL metastore.



\section{Data Processing Pipeline}

\textbf{Data Ingestion:} CSV to Parquet conversion with predefined schema for 6M+ rows, HDFS storage with compression, comprehensive data quality validation confirming 6,362,620 records with zero missing values.

% Image placeholder for data schema
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/data_schema.png}
    \caption{Data Schema and Sample Records}
    \label{fig:data_schema}
\end{figure}

\section{Exploratory Data Analysis \& Feature Engineering}

\textbf{Key EDA Findings:}
\begin{itemize}
    \item \textbf{Class Imbalance:} 6,354,407 normal (99.87\%) vs 8,213 fraud (0.13\%) transactions
    \item \textbf{Fraud Distribution:} Exclusively in TRANSFER (4,097 cases) and CASH\_OUT (4,116 cases) transaction types
    \item \textbf{Temporal Patterns:} Fraud distribution varies across 24-hour periods with identifiable peak hours
\end{itemize}

% Class Distribution Visualization
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/class_distribution.png}
    \caption{Class Distribution (Fraud vs. Normal) - Showing extreme class imbalance with 99.87\% normal and 0.13\% fraud transactions}
    \label{fig:class_distribution}
\end{figure}

% Fraud by Transaction Type
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/fraud_by_transaction_type.png}
    \caption{Fraud Analysis by Transaction Type - Fraud occurs exclusively in CASH\_OUT and TRANSFER transactions}
    \label{fig:fraud_by_type}
\end{figure}

% Transaction Amount Distribution
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/amount_distribution.png}
    \caption{Log(1 + Amount) Distribution by Class - Fraudulent transactions show different distribution patterns compared to normal transactions}
    \label{fig:amount_distribution}
\end{figure}

% Correlation Analysis
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/correlation_heatmap.png}
    \caption{Correlation Heatmap - Feature relationships showing strong correlations between balance variables}
    \label{fig:correlation_heatmap}
\end{figure}

% Temporal Analysis - Fraud over time
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/fraud_over_time.png}
    \caption{Fraudulent Transactions Over Time (by Step/Hour) - Temporal patterns in fraud occurrence across the 30-day simulation}
    \label{fig:fraud_over_time}
\end{figure}

% Fraud by Hour of Day
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/fraud_by_hour.png}
    \caption{Total Fraudulent Transactions by Hour of Day (UTC) - Identifying peak fraud hours and daily patterns}
    \label{fig:fraud_by_hour}
\end{figure}

\textbf{Feature Engineering:}
\begin{itemize}
    \item \textbf{Feature Selection:} Retained 8 core features, removed high-cardinality identifiers
    \item \textbf{Temporal Features:} Hour of day extracted from step variable
    \item \textbf{Encoding:} StringIndexer + OneHotEncoder for transaction types
    \item \textbf{Scaling:} VectorAssembler + StandardScaler for final 12-dimensional feature space
    \item \textbf{Data Split:} 80\% training (5M records), 20\% testing (1.3M records)
    \item \textbf{Class Balance:} SMOTE-like oversampling for balanced training set
\end{itemize}

\section{Machine Learning Models \& Results}

\textbf{Models Implemented:}
\begin{itemize}
    \item \textbf{Random Forest:} 50 trees, max depth 10, subsampling 0.8, local filesystem persistence
    \item \textbf{Gradient Boosted Trees:} 50 iterations, max depth 8, step size 0.1, memory-optimized
\end{itemize}

\textbf{Model Persistence Framework:} Automatic model existence checking, local filesystem storage, intelligent loading optimization, comprehensive error handling.

% Model performance visualization
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/model_performance.png}
    \caption{Model Performance Comparison and ROC/PR Curves}
    \label{fig:model_performance}
\end{figure}

\textbf{Evaluation Results:} Both models achieved excellent fraud detection performance using comprehensive metrics (Accuracy, Precision, Recall, F1-Score, ROC-AUC, PR-AUC). GBT showed best overall performance with balanced accuracy and training efficiency.

\section{Conclusion \& Business Impact}

This project successfully demonstrates distributed machine learning for large-scale fraud detection with the following achievements:

\textbf{Technical Accomplishments:}
\begin{itemize}
    \item Processed 6.3+ million transaction records using distributed Hadoop/Spark infrastructure
    \item Implemented two high-performance ML models with automatic persistence and loading
    \item Achieved excellent fraud detection performance despite extreme class imbalance (774:1)
    \item Created production-ready deployment pipeline with comprehensive monitoring
\end{itemize}

\textbf{Business Value:}
\begin{itemize}
    \item Real-time fraud detection capability reducing manual review costs
    \item Scalable infrastructure handling growing transaction volumes
    \item Proactive fraud prevention protecting financial institutions
    \item Audit trail and model explainability supporting regulatory compliance
\end{itemize}

\section*{References}
\begin{enumerate}
    \item PaySim Dataset: \url{https://www.kaggle.com/datasets/ealaxi/paysim1}
    \item Apache Spark Documentation: \url{https://spark.apache.org/docs/}
    \item PySpark MLlib Guide: \url{https://spark.apache.org/docs/latest/ml-guide.html}
\end{enumerate}

\end{document}